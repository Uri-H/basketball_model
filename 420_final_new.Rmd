---
title: "Predictive Modeling of NBA Player Performance Metrics"
author: "Uri Hoch"
date: "2023-08-03"
output:
  html_document: 
    theme: readable
    toc: yes
  pdf_document: default
urlcolor: cyan
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
In this project, we analyze a dataset of National Basketball Association (NBA) player statistics sourced from NBA.com and basketball-reference.com (see links at bottome of page). Our aim is to derive a model that can effectively predict a player's net +/- rating, a metric indicative of a player's impact on the court.

The dataset contains various details of NBA players' performances, such as their points scored (PTS), field goal percentage (FG.%), three-point percentage (X3P.%), free throw attempts (FTA), offensive rebounds (OREB), defensive rebounds (DREB), assists (AST), turnovers (TOV), steals (STL), blocks (BLK), personal fouls (PF), and player position.

In basketball, players need to excel in several areas to contribute positively to their team's performance. Understanding the combination of statistics that best predicts a player's impact can provide valuable insights for team management in player evaluation, strategic decision-making, and overall team composition.

We will attempt to create a model that can predict a player's net rating based on their individual performance metrics. We will use multiple linear regression modeling techniques and check LINE assumptions. We will address potential challenges such as multicollinearity and heteroscedasticity, if present. We will also evaluate our model's performance using metrics such as the Adjusted R-squared value and conduct diagnostics to identify potential outliers and influential points.

## Methods
```{r}
player_data = read.csv("nbaplayerstats.csv") 
team_data = read.csv("nbateamstats.csv")
position_data = read.csv("nba_position_data.csv")

library(dplyr)

position_data <- position_data %>%
  group_by(First_Name, Last_Name) %>%
  summarise(Position = names(which.max(table(Position))))

player_data <- merge(player_data, position_data, by = c("First_Name", "Last_Name"), all.x = TRUE)

head(player_data)
summary(player_data)



na_positions <- sum(is.na(player_data$Position))

print(paste("Number of players without position: ", na_positions))

# Subset the data for players without position
#missing_positions <- player_data[is.na(player_data$Position),]

#print(missing_positions)
```

Now we have the data combined with all of our position data.

```{r}
colnames(player_data)
colnames(team_data)
```
Here we check the column names of our player data and team data so we know what parameters we are dealing with.

```{r}
team_name_mapping <- c(
  "ATL" = "Hawks",
  "BKN" = "Nets",
  "BOS" = "Celtics",
  "CHA" = "Hornets",
  "CHI" = "Bulls",
  "CLE" = "Cavaliers",
  "DAL" = "Mavericks",
  "DEN" = "Nuggets",
  "DET" = "Pistons",
  "GSW" = "Warriors",
  "HOU" = "Rockets",
  "IND" = "Pacers",
  "LAC" = "Clippers",
  "LAL" = "Lakers",
  "MEM" = "Grizzlies",
  "MIA" = "Heat",
  "MIL" = "Bucks",
  "MIN" = "Timberwolves",
  "NOP" = "Pelicans",
  "NYK" = "Knicks",
  "OKC" = "Thunder",
  "ORL" = "Magic",
  "PHI" = "76ers",
  "PHX" = "Suns",
  "POR" = "Trail Blazers",
  "SAC" = "Kings",
  "SAS" = "Spurs",
  "TOR" = "Raptors",
  "UTA" = "Jazz",
  "WAS" = "Wizards"
)

player_data$TEAM <- team_name_mapping[player_data$TEAM]

```

We modify the names of the teams in the player data to match the names of the teams in the team data.

```{r}
library(dplyr)
player_data <- player_data %>%
  mutate(per_game_rating = rating / GP)

team_data_small <- team_data %>%
  select(Team, Year, rating)

names(team_data_small) <- c("TEAM", "Year", "team_rating")

player_data <- left_join(player_data, 
                         team_data_small, 
                         by = c("TEAM" = "TEAM", "Year" = "Year"))

player_data <- player_data %>%
  mutate(player_net_rating = (per_game_rating - team_rating) * GP)

```

We create a new column called player_net_rating which calculates each player's +/- rating relative to his team's net +/- rating. This will insure that players on teasm's that are better don't get an advantage over player's on worse teams.

```{r}
# Subset data based on total minutes played condition
player_data <- player_data[player_data$MIN >= 100, ]
```

This removes players who played less than 100 minutes as these players are often statistical outliers having not played enough minutes for the law of large numbers to kick in.

```{r}
library(ggplot2)

variables <- c("AGE", "PTS", "FG.", "X3P.", "FT.", "OREB", "DREB", "AST", "TOV", "STL", "BLK", "PF", "Position")

for (var in variables) {
  p = ggplot(player_data, aes_string(x = var, y = "rating")) +
    geom_point() + 
    labs(x = var, y = "Player Net Rating") + 
    theme_minimal() + 
    ggtitle(paste("Scatterplot of", var, "vs Player Net Rating")) 
  
  print(p)
}

```

These scatterplots show the relationship between our parameters and the player net ratings. This will help us determine some of the transformations we should try on our parameters.


```{r}
full_model = lm(player_net_rating ~ AGE+PTS+I(FG.^2)+I(X3P.^2)+I(FT.^2)+OREB+DREB+AST+STL+BLK+as.factor(Position)+TOV+PF, data=player_data)
new_model = lm(I(player_net_rating^(1/3)) ~ I(1/(AGE^2))+PTS+I(FG.^2)+I(X3P.^2)+I(FT.^2)+DREB+AST+OREB+STL+BLK, data=player_data)
model = new_model
#model = lm(player_net_rating ~ .-First_Name-Last_Name-TEAM-Year-rating-per_game_rating-team_rating-L-FTM-REB-W-FP-DD2-TD3-MIN-FGA-FGM-X3PA-X3PM-FTA-GP, data = player_data)
#summary(model)
```
```{r}
library(car)
vif(model)
```

We started with the mostly full model that removed the columns that weren't helpful to our goal of predicting player net rating. Using player_net_rating~.-First_Name-Last_Name-TEAM-Year-rating-per_game_rating-team_rating. We also removed categories wins, minutes, games played etc. which were not helpful from a prediction standpoint. We also then ran vif() and found a bunch of parameters that overlapped with each other causing multicollinearity issues. Such as total rebounds including offensive and defensive rebounds combined.

We didn't want team to impact our results, the last two were used for creating the response and the others were essentially row identifiers. That left us with the full model above. After looking at the scatterplots though we decided to remove turnovers and personal fouls because what was happening is that players with higher turnovers and personal fouls actually had higher net ratings because these were the players that play a lot and have more opportunity to make mistakes or in some cases commit valuable fouls. The player's that play a lot also tend to be the best players so we realized that we should leave those two out since the relationship was deceptive. The position category did have some value but we remove it here because we were more interested in what statistical categories a player could contribute to than what position they happen to marked as by their team. We will also ultimately reacquire the position information circumstantially if for example we discover that rebounds is highly predictive for net rating than we would know that centers provide outsize value because they are the ones who are often the bulk rebounders.

The other changes we made were to transform the three shooting percentages to powers of 2 because the scatterplots show a nonlinear relationship with net rating. We also did a transformation of the age because we could see that the youngest player had the most improvement as they got older, then we reached an almost plateau and the oldest players started to lose value as they aged (at a slower rate than the younger players gained so we also considered a log relationship). These older players then tended to retire before their value presumably would have dropped off even more.
The last thing we did was modify the response to take the third root of the player net rating. We did this because our Q-Q plot and our fitted vs residuals plot were wildly off and were violating all our assumptions.

```{r}
library(ggplot2)
residuals = resid(model)
predicted = predict(model)

#show the fitted vs redisuals
plot(fitted(model), resid(model), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)

#show the Q-Q plot
qqnorm(resid(model), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(model), col = "dodgerblue", lwd = 2)

cooksd = cooks.distance(model)
plot(cooksd, pch="*", cex=2, main="Influence of Observations (Cook's distance)")
abline(h = 4*mean(cooksd, na.rm=T), col="red")

sum(cooks.distance(model) > 4 / length(cooks.distance(model)))
```

Here we test the assumptions of our model and we can see some fairly good results. For linearity we used our scatterplots to check which parameters did not have linear relationships and transformed the ones that didn't. Our fitted vs residuals plot looks mostly evenly scattered with maybe a handful of outliers but much better than our previous attempts and it the width of the points is about the same too. So we should feel good about homoscedasticity and independence now. In the Normal Q-Q plot our data adheres very close to our line, much better than our previous attempts. We can see very little tail which is good. So we can be fairly confident normality is not violated. 

Let's see if we can find a good model that uses fewer of our parameters:

```{r}
library(MASS)
null_model <- lm(player_net_rating ~ 1, data = player_data)

scope <- list(lower = null_model, upper = model)

backward_aic <- step(model, direction = "backward", trace = FALSE)
forward_aic <- step(null_model, scope = scope, direction = "forward", trace = FALSE)
both_aic <- step(model, scope = scope, direction = "both", trace = FALSE)

backward_bic <- step(model, k = log(nrow(player_data)), direction = "backward", trace = FALSE)
forward_bic <- step(model, scope = scope, k = log(nrow(player_data)), direction = "forward", trace = FALSE)
both_bic <- step(model, scope = scope, k = log(nrow(player_data)), direction = "both", trace = 0)

print(backward_aic) 
print(forward_aic) 
print(both_aic) 

print(backward_bic) 
print(forward_bic)
print(both_bic) 
```

We can see we have a lot of models that include steals and offensive rebounds and three point percentage also appear a couple times. We will now try including an interaction between steals and offensive rebounds. 


```{r}
new_model = lm(I(player_net_rating^(1/3)) ~ log(AGE)+PTS+I(FG.^2)+I(X3P.^2)+I(FT.^2)+DREB+AST+OREB*STL+BLK, data=player_data)
model = new_model
library(MASS)
null_model <- lm(player_net_rating ~ 1, data = player_data)

scope <- list(lower = null_model, upper = model)

backward_aic <- step(model, direction = "backward", trace = FALSE)
forward_aic <- step(null_model, scope = scope, direction = "forward", trace = FALSE)
both_aic <- step(model, scope = scope, direction = "both", trace = FALSE)

backward_bic <- step(model, k = log(nrow(player_data)), direction = "backward", trace = FALSE)
forward_bic <- step(model, scope = scope, k = log(nrow(player_data)), direction = "forward", trace = FALSE)
both_bic <- step(model, scope = scope, k = log(nrow(player_data)), direction = "both", trace = 0)

print(backward_aic) 
print(forward_aic) 
print(both_aic) 

print(backward_bic) 
print(forward_bic)
print(both_bic) 
```

We ran these one more time including an interaction this time between steals and offensive rebounds and we got some models than include just those three parameters. We tried adding and subtracting the various shot percentage stats but none made a significant enough impact to include them. So leaving those out we now check our this smaller model against the new model from above.

```{r}
new_model = lm(I(player_net_rating^(1/3)) ~ log(AGE)+PTS+I(FG.^2)+I(X3P.^2)+I(FT.^2)+DREB+AST+OREB*STL+BLK, data=player_data)
model4= lm(formula = I(player_net_rating^(1/3)) ~ OREB*STL, data = player_data)
summary(model4)
anova(model4,new_model)
```

We see with only these three parameters we get a large p value from the anova test that has us preferring the smaller model. Our individual paramters also all have p values of significance. Our R squared values aren't particularly high but that is the trade off for the leaner model.

```{r}
hat_values <- hatvalues(model4)
high_leverage_points_indices <- which(hat_values > 2*length(coef(model4))/nrow(player_data))  # high leverage points
num_high_leverage_points <- sum(hat_values > 2*length(coef(model))/nrow(player_data))

cooks_D <- cooks.distance(model4)
influential_outlier_indices <- which(cooks_D > 4/nrow(player_data))  # influential points
num_influential_outliers <- sum(cooks_D > 4/nrow(player_data))
#print(influential_outlier_indices)

studentized_residuals <- rstudent(model4)
outlier_indices <- which(abs(studentized_residuals) > 3)
num_outliers <- sum(abs(studentized_residuals) > 3)


print(paste("Number of outliers:", num_outliers))
print(paste("Number of influential outliers:", num_influential_outliers))
print(paste("Number of high leverage points:", num_high_leverage_points))
```

Here we check our influential points, outlier points and high leverage points. This is how we knew it made sense to remove players with less than 100 minutes played because they were outliers in the dataset. For example, some players had a 0% field goal percentage which makes sense because if a player doesn't take a lot of shots they might miss the one or two that they do take, which isn't a good representative sample of a player's abilities.

## Result

```{r}
model = model4
library(ggplot2)
residuals = resid(model)
predicted = predict(model)

#show the fitted vs redisuals
plot(fitted(model), resid(model), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)

#show the Q-Q plot
qqnorm(resid(model), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(model), col = "dodgerblue", lwd = 2)

cooksd = cooks.distance(model)
plot(cooksd, pch="*", cex=2, main="Influence of Observations (Cook's distance)")
abline(h = 4*mean(cooksd, na.rm=T), col="red")

sum(cooks.distance(model) > 4 / length(cooks.distance(model)))
```

We now check the assumptions of our smaller model and they are even better than our previous assumption checks.

## Discussion

We end up with a model that indicates that steals and offensive rebounds are the best predictors for how a player contributes to their team's success via their net rating. This make quite a lot of sense. The two stats have a major commonality. They both give their team an extra possession. An extra possession means a full opportunity to score again at any possible point value. The stats like the shot percentage stats are useful but not as much. If we think about it from an expected value perspective: let's say league average field goal percentage is 50%. If a player gives their team an extra possession and they take a two point shot they gain 1 expected point. Versus not having an extra possession but instead boosting field goal percentage from 50% to a DeAndre Jordan like 76.3% in 2020-21. On a two point shot that would be worth an additional 0.526 points, less than the 1 we gained from our steal/offensive rebound.

Should NBA teams only go after players with high steal and offensive rebound totals? There a lot of other factors to consider in team building and improving shot percentage for example still matters (especially at the margins in close games) just maybe less so than these two categories do in the long run. So a better takeaway would be that there is significant value to gain from picking players that are better at steals and offensive rebounds than other similar players at their position and with similar stats.

## Appendix

Data from:
https://www.nba.com/stats/players/traditional?PerMode=Totals&Season=2020-21&SeasonType=Regular%20Season&dir=A&sort=FG_PCT
https://www.nba.com/stats/teams/traditional?SeasonType=Regular+Season&Season=2019-20
and
https://www.basketball-reference.com/
https://www.basketball-reference.com/leagues/NBA_2023_totals.html 
